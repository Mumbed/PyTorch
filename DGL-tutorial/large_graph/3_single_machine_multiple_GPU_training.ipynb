{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 다중 GPU를 사용한 GNN의 확률적(Storchastic) 학습 \n",
    "\n",
    "\n",
    "이번 튜토리얼에서는 Multi GPU 환경에서 노드 분류를 위한 다중 레이어 GraphSAGE 모델을 학습하는 방법을 배워보겠습니다.  \n",
    "사용할 데이터셋은 OGB에서 제공하는 Amazon Copurchase Network으로, 240만 노드와 6100만 엣지를 포함하고 있으므로, 단일 GPU에는 올라가지 않습니다.  \n",
    "\n",
    "\n",
    "이 튜토리얼은 다음 내용을 포함하고 있습니다.  \n",
    "\n",
    "* `torch.nn.parallel.DistributedDataParallel` 메서드를 사용해 그래프 크기에 상관없이 GNN 모델을 단일 머신, 다중 GPU으로 학습하기.\n",
    "\n",
    "PyTorch `DistributedDataParallel` (혹은 짧게 말해 DDP)는 multi-GPU 학습의 일반적인 해결책입니다.  \n",
    "DGL과 PyTorch DDP를 결합하는 것은 매우 쉬운데, 평범한 PyTorch 어플리케이션에서 적용하는 방법과 같이 하면 됩니다.\n",
    "\n",
    "* 데이터를 각 GPU에 대해 분할하기\n",
    "* PyTorch DDP를 사용해 모델 파라미터를 분배합니다\n",
    "* 이웃 샘플링 전략을 각자의 방법으로 커스터마이징합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import dgl\n",
    "import torch\n",
    "import dgl.nn as dglnn\n",
    "import torch.nn as nn\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import torch.nn.functional as F\n",
    "import torch.multiprocessing as mp\n",
    "import sklearn.metrics\n",
    "import tqdm\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 로드하기\n",
    "\n",
    "아래 코드는 첫번째 튜토리얼에서 복사되었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    import pickle\n",
    "\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "    utils.prepare_mp(graph)\n",
    "    \n",
    "    num_features = node_features.shape[1]\n",
    "    num_classes = (node_labels.max() + 1).item()\n",
    "    \n",
    "    return graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이웃 샘플링 커스터마이징하기\n",
    "\n",
    "이전 튜토리얼에서, `NodeDataLoader`와 `MultiLayerNeighborSampler`를 사용하는 방법을 배워 보았습니다.  \n",
    "사실, `MultiLayerNeighborSampler`를 우리 마음대로 정한 샘플링 전략으로 대체할 수 있습니다.  \n",
    "\n",
    "커스터마이징은 간단합니다.  \n",
    "각 GNN 레이어에 대해, message passing에서 포함되는 엣지를 그래프로 지정해주면 됩니다.  \n",
    "이 그래프는 기존 그래프와 같은 노드를 갖게 됩니다.  \n",
    "\n",
    "예를 들어, `MultiLayerNeighborSampler`는 아래와 같이 구현됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerNeighborSampler(dgl.dataloading.BlockSampler):\n",
    "    def __init__(self, fanouts):\n",
    "        super().__init__(len(fanouts), return_eids=False)\n",
    "        self.fanouts = fanouts\n",
    "        \n",
    "    def sample_frontier(self, layer_id, g, seed_nodes):\n",
    "        fanout = self.fanouts[layer_id]\n",
    "        return dgl.sampling.sample_neighbors(g, seed_nodes, fanout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel (DDP)를 위한 데이터 로더 정의하기\n",
    "\n",
    "PyTorch DDP에서, 각 worker process는 정수값인 *rank*로 할당됩니다.  \n",
    "이 rank는 worker process가 데이터셋의 어떤 파티션을 처리할지를 나타냅니다.  \n",
    "\n",
    "따라서 데이터 로더 관점에서의 단일 GPU 경우와 다중 GPU 학습 간 유일한 차이점은,  \n",
    "데이터 로더가 노드의 일부 파티션에 대해서만 iterate한다는 점입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(rank, world_size, graph, nids):\n",
    "    partition_size = len(nids) // world_size\n",
    "    partition_offset = partition_size * rank\n",
    "    nids = nids[partition_offset:partition_offset+partition_size]\n",
    "    \n",
    "    sampler = MultiLayerNeighborSampler([4, 4, 4])\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nids, sampler,\n",
    "        batch_size=1024,\n",
    "        shuffle=True,\n",
    "        drop_last=False,\n",
    "        num_workers=0\n",
    "    )\n",
    "    \n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 정의하기\n",
    "\n",
    "모델 구현은 첫번째 튜토리얼에서 본 것과 정확히 동일합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_classes, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers - 1):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        self.layers.append(dglnn.SAGEConv(n_hidden, n_classes, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델을 여러 GPU에 분배하기\n",
    "\n",
    "PyTorch DDP는 모델의 분산과 가중치의 synchronization을 관리해 줍니다.  \n",
    "DGL에서는, 모델을 단순히 `torch.nn.parallel.DistributedDataParallel`으로 감싸 줌으로써 이 PyTorch DDP의 이점을 그대로 누릴 수 있습니다.\n",
    "\n",
    "분산 학습에서 추천되는 방식은 한 GPU에 학습 process를 하나만 가져가는 것입니다.  \n",
    "이로써, 모델 instantiation 중에 process rank를 지정해줄 수도 있게 되는데, 이 rank가 GPU ID와 동일해지게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(rank, in_feats, n_hidden, n_classes, n_layers):\n",
    "    model = SAGE(in_feats, n_hidden, n_classes, n_layers).to(rank)\n",
    "    return DistributedDataParallel(model, device_ids=[rank], output_device=rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1개 process를 위한 학습 루프\n",
    "\n",
    "학습 루프는 다른 PyTorch DDP 어플리케이션과 똑같이 생겼습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utils.fix_openmp\n",
    "def train(rank, world_size, data):\n",
    "    # data is the output of load_data\n",
    "    torch.distributed.init_process_group(\n",
    "        backend='nccl',\n",
    "        init_method='tcp://127.0.0.1:12345',\n",
    "        world_size=world_size,\n",
    "        rank=rank)\n",
    "    torch.cuda.set_device(rank)\n",
    "    \n",
    "    graph, node_features, node_labels, train_nids, valid_nids, test_nids, num_features, num_classes = data\n",
    "    \n",
    "    train_dataloader = create_dataloader(rank, world_size, graph, train_nids)\n",
    "    # We only use one worker for validation\n",
    "    valid_dataloader = create_dataloader(0, 1, graph, valid_nids)\n",
    "    \n",
    "    model = init_model(rank, num_features, 128, num_classes, 3)\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    torch.distributed.barrier()\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    best_model_path = 'model.pt'\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "\n",
    "        for step, (input_nodes, output_nodes, bipartites) in enumerate(train_dataloader):\n",
    "            bipartites = [b.to(rank) for b in bipartites]\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            labels = node_labels[output_nodes].cuda()\n",
    "            predictions = model(bipartites, inputs)\n",
    "\n",
    "            loss = F.cross_entropy(predictions, labels)\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            accuracy = sklearn.metrics.accuracy_score(labels.cpu().numpy(), predictions.argmax(1).detach().cpu().numpy())\n",
    "\n",
    "            if rank == 0 and step % 10 == 0:\n",
    "                print('Epoch {:05d} Step {:05d} Loss {:.04f}'.format(epoch, step, loss.item()))\n",
    "\n",
    "        torch.distributed.barrier()\n",
    "        \n",
    "        if rank == 0:\n",
    "            model.eval()\n",
    "            predictions = []\n",
    "            labels = []\n",
    "            with torch.no_grad():\n",
    "                for input_nodes, output_nodes, bipartites in valid_dataloader:\n",
    "                    bipartites = [b.to(rank) for b in bipartites]\n",
    "                    inputs = node_features[input_nodes].cuda()\n",
    "                    labels.append(node_labels[output_nodes].numpy())\n",
    "                    predictions.append(model.module(bipartites, inputs).argmax(1).cpu().numpy())\n",
    "                predictions = np.concatenate(predictions)\n",
    "                labels = np.concatenate(labels)\n",
    "                accuracy = sklearn.metrics.accuracy_score(labels, predictions)\n",
    "                print('Epoch {} Validation Accuracy {}'.format(epoch, accuracy))\n",
    "                if best_accuracy < accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    torch.save(model.module.state_dict(), best_model_path)\n",
    "                    \n",
    "        torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 Step 00000 Loss 5.7553\n",
      "Epoch 00000 Step 00010 Loss 2.6858\n",
      "Epoch 00000 Step 00020 Loss 2.1455\n",
      "Epoch 00000 Step 00030 Loss 1.7148\n",
      "Epoch 00000 Step 00040 Loss 1.6470\n",
      "Epoch 0 Validation Accuracy 0.7247158151717824\n",
      "Epoch 00001 Step 00000 Loss 1.3390\n",
      "Epoch 00001 Step 00010 Loss 1.3108\n",
      "Epoch 00001 Step 00020 Loss 1.3176\n",
      "Epoch 00001 Step 00030 Loss 1.4312\n",
      "Epoch 00001 Step 00040 Loss 1.1797\n",
      "Epoch 1 Validation Accuracy 0.7972687739999491\n",
      "Epoch 00002 Step 00000 Loss 1.0574\n",
      "Epoch 00002 Step 00010 Loss 1.1461\n",
      "Epoch 00002 Step 00020 Loss 1.0746\n",
      "Epoch 00002 Step 00030 Loss 1.0027\n",
      "Epoch 00002 Step 00040 Loss 0.9308\n",
      "Epoch 2 Validation Accuracy 0.8152480736464665\n",
      "Epoch 00003 Step 00000 Loss 0.9768\n",
      "Epoch 00003 Step 00010 Loss 1.0767\n",
      "Epoch 00003 Step 00020 Loss 0.9237\n",
      "Epoch 00003 Step 00030 Loss 1.0979\n",
      "Epoch 00003 Step 00040 Loss 0.8528\n",
      "Epoch 3 Validation Accuracy 0.83111664928922\n",
      "Epoch 00004 Step 00000 Loss 0.9134\n",
      "Epoch 00004 Step 00010 Loss 0.9284\n",
      "Epoch 00004 Step 00020 Loss 0.8158\n",
      "Epoch 00004 Step 00030 Loss 0.9542\n",
      "Epoch 00004 Step 00040 Loss 0.9215\n",
      "Epoch 4 Validation Accuracy 0.839508684484907\n",
      "Epoch 00005 Step 00000 Loss 0.9607\n",
      "Epoch 00005 Step 00010 Loss 0.9081\n",
      "Epoch 00005 Step 00020 Loss 0.8607\n",
      "Epoch 00005 Step 00030 Loss 0.8400\n",
      "Epoch 00005 Step 00040 Loss 0.8883\n",
      "Epoch 5 Validation Accuracy 0.8434249675762276\n",
      "Epoch 00006 Step 00000 Loss 0.7871\n",
      "Epoch 00006 Step 00010 Loss 0.9050\n",
      "Epoch 00006 Step 00020 Loss 0.8587\n",
      "Epoch 00006 Step 00030 Loss 0.7345\n",
      "Epoch 00006 Step 00040 Loss 0.7846\n",
      "Epoch 6 Validation Accuracy 0.8497317091778348\n",
      "Epoch 00007 Step 00000 Loss 0.7165\n",
      "Epoch 00007 Step 00010 Loss 0.8370\n",
      "Epoch 00007 Step 00020 Loss 0.8072\n",
      "Epoch 00007 Step 00030 Loss 0.7852\n",
      "Epoch 00007 Step 00040 Loss 0.8651\n",
      "Epoch 7 Validation Accuracy 0.853012232027058\n",
      "Epoch 00008 Step 00000 Loss 0.8609\n",
      "Epoch 00008 Step 00010 Loss 0.6784\n",
      "Epoch 00008 Step 00020 Loss 0.7328\n",
      "Epoch 00008 Step 00030 Loss 0.8150\n",
      "Epoch 00008 Step 00040 Loss 0.8347\n",
      "Epoch 8 Validation Accuracy 0.852732497520535\n",
      "Epoch 00009 Step 00000 Loss 0.7051\n",
      "Epoch 00009 Step 00010 Loss 0.7738\n",
      "Epoch 00009 Step 00020 Loss 0.8157\n",
      "Epoch 00009 Step 00030 Loss 0.7437\n",
      "Epoch 00009 Step 00040 Loss 0.7249\n",
      "Epoch 9 Validation Accuracy 0.8549703735727182\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    procs = []\n",
    "    data = load_data()\n",
    "    for proc_id in range(4):    # 4 gpus\n",
    "        p = mp.Process(target=train, args=(proc_id, 4, data))\n",
    "        p.start()\n",
    "        procs.append(p)\n",
    "    for p in procs:\n",
    "        p.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "이 튜토리얼에서, GPU에 올라가지 않는 대규모 데이터에서 노드 분류를 위한 다중 레이어 GraphSAGE 모델을 학습하는 방법을 배웠습니다.  \n",
    "여기서 배운 이 방법은 어떤 사이즈의 그래프에서든 확장될 수 있으며,  \n",
    "단일 머신의 *몇 개의 GPU 에서든* 작동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추가 자료: DDP로 학습할 때의 주의점\n",
    "\n",
    "DDP 코드를 작성할 때, 이 두가지 에러를 겪을 수 있습니다.  \n",
    "\n",
    "* `Cannot re-initialize CUDA in forked subprocess`  \n",
    "\n",
    "    이는 `mp.Process`를 사용해 subprocess를 만들기 전에 CUDA context를 초기화 해서 발생합니다.\n",
    "    해결책은 다음과 같습니다.  \n",
    "    \n",
    "    * `mp.Process`를 호출하기 전에, CUDA context를 초기화할 수 있는 모든 가능한 코드를 제거합니다.  \n",
    "    예를 들어, `mp.Process`를 호출하기 전에 GPU의 갯수를 `torch.cuda.device_count()`로 확인할 수 없습니다.  \n",
    "    왜냐하면, 갯수를 확인하는 `torch.cuda.device_count()`는 CUDA context를 초기화하기 때문입니다.  \n",
    "    \n",
    "    CUDA context가 초기화 되었는지의 여부를 `torch.cuda.is_initialized()`로 확인해볼 수 있습니다.\n",
    "    \n",
    "    * `mp.Process`로 forking하지 마시고, `torch.multiprocessing.spawn()`를 사용해 process를 생성하세요.  \n",
    "    (전자 방식의) 불리점은, 파이썬이 이 방법으로 생성된 모든 process에 대해 그래프 storage를 복제한다는 점입니다.  \n",
    "    메모리 소비량이 선형적으로 증가하게 되지요.\n",
    "    \n",
    "* 학습 프로세스가 미니배치 iteration중에 멈춤\n",
    "    이 원인은 다음과 같습니다. [lasting bug in the interaction between GNU OpenMP and `fork`](https://github.com/pytorch/pytorch/issues/17199)  \n",
    "    다른 해결책은, `mp.Process`의 목표 함수를 데코레이터 `utils.fix_openmp`를 사용해 감싸는 것입니다.  \n",
    "    이 방식은 이 튜토리얼에서 구현되어 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
