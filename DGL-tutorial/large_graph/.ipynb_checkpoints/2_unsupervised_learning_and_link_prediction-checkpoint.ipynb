{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 대규모 그래프에서의 링크 예측을 위한 GNN의 확률적(Storchastic) 학습 \n",
    "\n",
    "이번 튜토리얼에서는, 다중 레이어 GraphSAGE 모델을 비지도학습 방식으로 학습시키는 방법을 OGB가 제공하는 Amazon Copurchase Netword 데이터의 링크 예측을 통해 배워봅니다.  \n",
    "데이터셋은 240만 노드와 6100만 엣지를 포함하고 있으며, 따라서 단일 GPU에 올라가지 않습니다.\n",
    "\n",
    "이 튜토리얼의 내용은 다음을 포함합니다.  \n",
    "\n",
    "* GNN 모델을 그래프 크기에 상관없이 1개의 GPU를 가진 단일 머신으로 학습하기  \n",
    "* 링크 예측 task를 수행하는 GNN 모델 학습하기\n",
    "* 비지도 학습을 위한 GNN 모델 학습하기\n",
    "\n",
    "이 튜토리얼은 이전의 튜토리얼에서 다운받은 데이터를 활용합니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link Prediction Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "링크 예측의 목표는 두개의 주어진 노드 사이에 엣지가 존재하는지를 예측하는 것입니다.  \n",
    "보통 이런 문제를 $s_{uv} = \\phi(\\boldsymbol{h}^{(l)}_u, \\boldsymbol{h}^{(l)}_v)$라는 점수를 예측하는 문제로 수식화 하는데요,  \n",
    "이는 두 노드 사이에 존재하는 엣지의 likelihood를 의미합니다.  \n",
    "\n",
    "또, 모델을 *네거티브 샘플링 negative sampling*을 통해 학습합니다. \n",
    "즉, 실재 존재하는 엣지와 \"존재하지 않는\" 엣지의 점수를 비교함으로써 학습한다는 의미입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적인 손실함수 중 하나는 negative log-likelihood 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathcal{L} = -\\log \\sigma\\left(s_{uv}\\right) - Q \\mathbb{E}_{v^- \\in P^-(v)}\\left[ \\sigma\\left(-s_{uv^-}\\right) \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BPR이나 margin loss와 같은 다른 손실함수를 사용할 수도 있습니다.  \n",
    "\n",
    "위의 수식이 implicit matrix factorization 혹은 워드 임베딩 학습과 비슷하다는 점에 주목해 주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN을 사용한 비지도학습의 개요\n",
    "\n",
    "링크 예측 그 자체는 한 노드가 다른 노드와 상호작용할지 예측하는 추천과 같은 다양한 작업에서 이미 유용성을 입증했습니다.  \n",
    "또 링크 예측은 모든 노드의 잠재 표현을 학습하고자 하는, 비지도 학습의 상황에서도 유용합니다.\n",
    "\n",
    "모델은 두 노드가 엣지로 연결 되어 있을지 아닐지를 예측하는 비지도 학습적인 방식으로 학습될 것이고,  \n",
    "학습된 표현은 최근접 이웃(nearest neighbor, NN) 검색 혹은 추후의 분류 모델 학습에 활용될 수 있겠죠.  \n",
    "\n",
    "또, 목적 함수는 노드 분류를 위한 지도학습의 cross-entropy loss와 결합될 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 데이터셋 로드하기\n",
    "\n",
    "이전 튜토리얼에서 전처리된 데이터셋을 직접 가져오겠습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import numpy as np\n",
    "import utils\n",
    "import pickle\n",
    "\n",
    "with open('data.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "graph, node_features, node_labels, train_nids, valid_nids, test_nids = data\n",
    "graph.create_formats_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 이웃 샘플링으로 데이터 로더 정의하기\n",
    "\n",
    "노드 분류와는 다르게, 엣지에 걸쳐 iterate해야합니다. 그 뒤 이웃 샘플링과 GNN을 사용해 해당 노드들의 출력 표현을 계산해야 합니다.  \n",
    "\n",
    "DGL은 `EdgeDataLoader`을 제공합니다. 이 메서드는 엣지 분류 혹은 링크 예측을 위해 엣지를 iterate하도록 도와줍니다.  \n",
    "\n",
    "링크 예측을 수행하기 위해, negative sampler를 제공해 주어야 합니다.  \n",
    "\n",
    "동질적(homogeneous) 그래프에서는, negative sample는 아래의 양식을 가진 어떤 callable 객체든 가능합니다.  \n",
    "\n",
    "```python\n",
    "def negative_sampler(g: DGLGraph, eids: Tensor) -> Tuple[Tensor, Tensor]:\n",
    "    pass\n",
    "```\n",
    "\n",
    "첫번째 인자는 원래 그래프이고, 두번째 인자는 엣지 ID의 미니배치를 의미합니다.  \n",
    "이 함수는  $u$-$v^-$ 노드 ID 텐서의 쌍을 negative example로 반환합니다. \n",
    "\n",
    "\n",
    "다음 코드는 `k`개의 $v^-$를 각 $u$에 대해 $P^-(v) \\propto d(v)^{0.75}$의 분포를 따라 샘플링 함으로써,\n",
    "그래프 내에 존재하지 않는 엣지를 찾는 negative sampler 기능을 수행합니다.  \n",
    "여기서 $d(v)$는 $v$의 차수(degree)를 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NegativeSampler(object):\n",
    "    def __init__(self, g, k):\n",
    "        self.k = k\n",
    "        self.weights = g.in_degrees().float() ** 0.75\n",
    "    def __call__(self, g, eids):\n",
    "        src, _ = g.find_edges(eids)\n",
    "        src = src.repeat_interleave(self.k)\n",
    "        dst = self.weights.multinomial(len(src), replacement=True)\n",
    "        return src, dst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negative sampler를 정의한 뒤, edge 데이터 로더를 이웃 샘플링으로 정의할 수 있습니다.  \n",
    "여기서는 1개의 positive example에 대해 5개의 negative example을 만들어 주겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = dgl.dataloading.MultiLayerNeighborSampler([4, 4, 4])\n",
    "k = 5\n",
    "train_dataloader = dgl.dataloading.EdgeDataLoader(\n",
    "    graph, torch.arange(graph.number_of_edges()), sampler,\n",
    "    negative_sampler=NegativeSampler(graph, k),\n",
    "    batch_size=1024,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_dataloader`에서 미니배치 하나를 뜯어볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([1147853, 2426712,    1342,  ...,  292546,  134170,  102404]), Graph(num_nodes=7141, num_edges=1024,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}), Graph(num_nodes=7141, num_edges=5120,\n",
      "      ndata_schemes={'_ID': Scheme(shape=(), dtype=torch.int64)}\n",
      "      edata_schemes={}), [Block(num_src_nodes=230241, num_dst_nodes=112984, num_edges=415458), Block(num_src_nodes=112984, num_dst_nodes=33355, num_edges=126722), Block(num_src_nodes=33355, num_dst_nodes=7141, num_edges=28040)])\n"
     ]
    }
   ],
   "source": [
    "example_minibatch = next(iter(train_dataloader))\n",
    "print(example_minibatch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 예제 미니배치는 4개의 구성요소로 이루어져 있습니다.\n",
    "\n",
    "* 출력 노드의 표현을 계산하기 위해 필요한 입력 노드 리스트\n",
    "* 미니배치 내에서 샘플링된 노드에서 유도된 subgraph (negative example의 노드 포함)와 미니배치 내에서 샘플링된 엣지들\n",
    "* 미니배치 내에서 샘플링된 노드에서 유도된 subgraph (negative example의 노드 포함)와 negative sampler에서 샘플링된 존재하지 않는 엣지들\n",
    "* bipartite 그래프의 리스트, 각 레이어마다 하나씩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input nodes: 230241\n",
      "Positive graph # nodes: 7141 # edges: 1024\n",
      "Negative graph # noeds: 7141 # edges: 5120\n",
      "[Block(num_src_nodes=230241, num_dst_nodes=112984, num_edges=415458), Block(num_src_nodes=112984, num_dst_nodes=33355, num_edges=126722), Block(num_src_nodes=33355, num_dst_nodes=7141, num_edges=28040)]\n"
     ]
    }
   ],
   "source": [
    "input_nodes, pos_graph, neg_graph, bipartites = example_minibatch\n",
    "print('Number of input nodes:', len(input_nodes))\n",
    "print('Positive graph # nodes:', pos_graph.number_of_nodes(), '# edges:', pos_graph.number_of_edges())\n",
    "print('Negative graph # noeds:', neg_graph.number_of_nodes(), '# edges:', neg_graph.number_of_edges())\n",
    "print(bipartites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 노드 표현을 위한 모델 정의\n",
    "\n",
    "모델은 아래와 같이 정의됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.nn as dglnn\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self, in_feats, n_hidden, n_layers):\n",
    "        super().__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(dglnn.SAGEConv(in_feats, n_hidden, 'mean'))\n",
    "        for i in range(1, n_layers):\n",
    "            self.layers.append(dglnn.SAGEConv(n_hidden, n_hidden, 'mean'))\n",
    "        \n",
    "    def forward(self, bipartites, x):\n",
    "        for l, (layer, bipartite) in enumerate(zip(self.layers, bipartites)):\n",
    "            x = layer(bipartite, x)\n",
    "            if l != self.n_layers - 1:\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN에서 노드 표현 얻기\n",
    "\n",
    "이전 튜토리얼에서는, 이웃 샘플링 없이 GNN 모델의 offline 추론을 수행하는 것에 대해 이야기 했었죠.  \n",
    "그 방법을 그대로 복붙해서, 비지도 학습 환경에서의 GNN으로부터 노드 표현 출력값을 계산하는 데 사용할 수 있겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, graph, input_features, batch_size):\n",
    "    nodes = torch.arange(graph.number_of_nodes())\n",
    "    \n",
    "    sampler = dgl.dataloading.MultiLayerNeighborSampler([None])  # one layer at a time, taking all neighbors\n",
    "    dataloader = dgl.dataloading.NodeDataLoader(\n",
    "        graph, nodes, sampler,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for l, layer in enumerate(model.layers):\n",
    "            # Allocate a buffer of output representations for every node\n",
    "            # Note that the buffer is on CPU memory.\n",
    "            output_features = torch.zeros(graph.number_of_nodes(), model.n_hidden)\n",
    "\n",
    "            for input_nodes, output_nodes, bipartites in tqdm.tqdm(dataloader):\n",
    "                bipartite = bipartites[0].to(torch.device('cuda'))\n",
    "\n",
    "                x = input_features[input_nodes].cuda()\n",
    "\n",
    "                # the following code is identical to the loop body in model.forward()\n",
    "                x = layer(bipartite, x)\n",
    "                if l != model.n_layers - 1:\n",
    "                    x = F.relu(x)\n",
    "\n",
    "                output_features[output_nodes] = x.cpu()\n",
    "            input_features = output_features\n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엣지 스코어 예측 모델 정의하기\n",
    "\n",
    "미니 배치에서 필요한 노드 표현을 얻은 위에는,  \n",
    "샘플링된 미니 배치의 존재하는/존재하지 않는 엣지에 대한 스코어를 예측하고 싶겠죠?  \n",
    "\n",
    "이는 `apply_edges` 메서드로 쉽게 구현할 수 있습니다.  \n",
    "여기서는, 두 대상 노드의 표현의 내적을 계산함으로써 단순히 예산할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScorePredictor(nn.Module):\n",
    "    def forward(self, subgraph, x):\n",
    "        with subgraph.local_scope():\n",
    "            subgraph.ndata['x'] = x\n",
    "            subgraph.apply_edges(dgl.function.u_dot_v('x', 'x', 'score'))\n",
    "            return subgraph.edata['score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습된 임베딩의 성능 평가하기\n",
    "\n",
    "이 튜토리얼에서, 출력 임베딩을 학습 셋의 입력으로 사용해, 선형 분류 모델을 학습하여 출력 임베딩의 성능을 평가할 예정입니다.  \n",
    "그 뒤, 검증/테스트 셋에 대해 정확도를 측정해 보겠습니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "def evaluate(emb, label, train_nids, valid_nids, test_nids):\n",
    "    classifier = sklearn.linear_model.LogisticRegression(solver='lbfgs', multi_class='multinomial', verbose=1, max_iter=1000)\n",
    "    classifier.fit(emb[train_nids], label[train_nids])\n",
    "    valid_pred = classifier.predict(emb[valid_nids])\n",
    "    test_pred = classifier.predict(emb[test_nids])\n",
    "    valid_acc = sklearn.metrics.accuracy_score(label[valid_nids], valid_pred)\n",
    "    test_acc = sklearn.metrics.accuracy_score(label[test_nids], test_pred)\n",
    "    return valid_acc, test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 학습 루프 정의하기\n",
    "\n",
    "다음 코드는 모델을 초기화하고 최적화기(optimizer)를 정의합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(node_features.shape[1], 128, 3).cuda()\n",
    "predictor = ScorePredictor().cuda()\n",
    "opt = torch.optim.Adam(list(model.parameters()) + list(predictor.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래는 비지도 학습과 평가를 수행하는 학습 루프로,  \n",
    "validation set에 대해 최적의 성능을 보이는 모델을 저장하는 기능도 포함하고 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 26699/60410 [1:45:31<2:11:56,  4.26it/s, loss=0.614]"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import sklearn.metrics\n",
    "\n",
    "best_accuracy = 0\n",
    "best_model_path = 'model.pt'\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    \n",
    "    with tqdm.tqdm(train_dataloader) as tq:\n",
    "        for step, (input_nodes, pos_graph, neg_graph, bipartites) in enumerate(tq):\n",
    "            bipartites = [b.to(torch.device('cuda')) for b in bipartites]\n",
    "            pos_graph = pos_graph.to(torch.device('cuda'))\n",
    "            neg_graph = neg_graph.to(torch.device('cuda'))\n",
    "            inputs = node_features[input_nodes].cuda()\n",
    "            outputs = model(bipartites, inputs)\n",
    "            pos_score = predictor(pos_graph, outputs)\n",
    "            neg_score = predictor(neg_graph, outputs)\n",
    "            \n",
    "            score = torch.cat([pos_score, neg_score])\n",
    "            label = torch.cat([torch.ones_like(pos_score), torch.zeros_like(neg_score)])\n",
    "            loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "            tq.set_postfix({'loss': '%.03f' % loss.item()}, refresh=False)\n",
    "        \n",
    "    model.eval()\n",
    "    emb = inference(model, graph, node_features, 16384)\n",
    "    valid_acc, test_acc = evaluate(emb.numpy(), node_labels.numpy())\n",
    "    print('Epoch {} Validation Accuracy {} Test Accuracy {}'.format(epoch, valid_acc, test_acc))\n",
    "    if best_accuracy < valid_acc:\n",
    "        best_accuracy = valid_acc\n",
    "        torch.save(model.state_dict(), best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 결론\n",
    "\n",
    "이 튜토리얼에서, 비지도 학습 방식으로 다중 레이어 GraphSAGE 모델을 학습하는 방법을 GPU에 올라가지 않는 대규모 데이터셋의 링크 예측을 통해 배워 보았습니다.  \n",
    "여기서 배운 이 방법은 어떤 사이즈의 그래프에 대해서도 확장될 수 있고, 단일 머신의 1개 GPU로도 작동합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음은 무엇을 배우나요?\n",
    "\n",
    "다음 튜토리얼은 학습 절차를 단일 머신의 다중 GPU에 대해 scale-out하는 방법에 대해 배웁니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
