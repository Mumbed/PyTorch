{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU를 사용해 학습 가속하기\n",
    "\n",
    "이번 튜토리얼에서, 다음을 배우게 됩니다.\n",
    "\n",
    "* 그래프와 피처 데이터를 GPU로 복사하는 방법\n",
    "* GNN 모델을 GPU 위에 학습하는 방법\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 그래프와 피처 데이터를 GPU에 복사\n",
    "\n",
    "먼저 이전 세션에 사용한 Zachery의 카라테 클럽 그래프와 노드 라벨를 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph(num_nodes=34, num_edges=156,\n",
      "      ndata_schemes={'club': Scheme(shape=(), dtype=torch.int64), 'club_onehot': Scheme(shape=(2,), dtype=torch.int64)}\n",
      "      edata_schemes={})\n"
     ]
    }
   ],
   "source": [
    "from tutorial_utils import load_zachery\n",
    "\n",
    "# ----------- 0. load graph -------------- #\n",
    "g = load_zachery()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 그래프와 모든 그래프의 피처 데이터는 CPU에 적재되어 있습니다. `to` API를 사용해 다른 연산장치로 복사해 보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cpu\n",
      "New device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "print('Current device:', g.device)\n",
    "g = g.to('cuda:0')\n",
    "print('New device:', g.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that features are also copied to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(g.ndata['club'].device)\n",
    "print(g.ndata['club_onehot'].device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN 모델을 GPU에 생성하기\n",
    "\n",
    "이 스텝은 CNN이나 RNN 모델을 GPU에 생성하는 것과 같습니다.  \n",
    "PyTorch에서, `to` API를 사용해 이를 수행할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1941,  0.3631, -0.2146, -0.0416, -0.3040],\n",
       "        [-0.2014,  0.3393, -0.1062, -0.2229,  0.3331],\n",
       "        [-0.2843,  0.3027,  0.2389, -0.0853,  0.3283],\n",
       "        [ 0.0482,  0.1986, -0.2904,  0.0818, -0.1860],\n",
       "        [-0.0844, -0.3876,  0.1654, -0.2600,  0.3482],\n",
       "        [ 0.1354, -0.1090,  0.0389,  0.2281, -0.2484],\n",
       "        [-0.3592,  0.1807, -0.2933, -0.2188,  0.2301],\n",
       "        [ 0.2413,  0.3598, -0.2222, -0.2795, -0.1307],\n",
       "        [-0.3511, -0.1753,  0.2872, -0.2322,  0.0094],\n",
       "        [ 0.1164,  0.0620,  0.0414, -0.1472, -0.2698],\n",
       "        [-0.1343,  0.3105, -0.3529,  0.3063, -0.1436],\n",
       "        [ 0.2085,  0.0502, -0.2477,  0.2870, -0.0683],\n",
       "        [-0.2910, -0.2569, -0.1903, -0.0875, -0.3270],\n",
       "        [ 0.1782,  0.2922, -0.2446, -0.0885, -0.3430],\n",
       "        [-0.0839,  0.0179,  0.2307, -0.1886,  0.0091],\n",
       "        [-0.2154, -0.2445,  0.2925,  0.1994, -0.3176],\n",
       "        [-0.0218, -0.1832, -0.2908,  0.3639, -0.3595],\n",
       "        [ 0.1016, -0.0547, -0.3834,  0.2332,  0.1355],\n",
       "        [ 0.2532, -0.2284,  0.1160,  0.2327, -0.2220],\n",
       "        [-0.1158,  0.0133,  0.1556,  0.1818, -0.1308],\n",
       "        [ 0.1331, -0.0689, -0.2183,  0.0959, -0.2642],\n",
       "        [ 0.1583,  0.0246, -0.1268,  0.3539, -0.3599],\n",
       "        [ 0.2765, -0.1886, -0.1546, -0.3603, -0.3806],\n",
       "        [ 0.3022,  0.0966,  0.0634, -0.3355,  0.1699],\n",
       "        [ 0.0687, -0.0457,  0.2138,  0.3206,  0.3198],\n",
       "        [-0.1122,  0.2960, -0.3739, -0.2899,  0.3898],\n",
       "        [ 0.1120,  0.2343, -0.2354, -0.1214,  0.3795],\n",
       "        [-0.3419,  0.0163, -0.2615,  0.1877,  0.0776],\n",
       "        [ 0.3821,  0.3670,  0.2761, -0.2352,  0.2398],\n",
       "        [ 0.3302, -0.1100, -0.3390,  0.2329,  0.0696],\n",
       "        [ 0.0951,  0.0089,  0.1248, -0.0494, -0.2868],\n",
       "        [ 0.0962,  0.1329,  0.2705, -0.0595,  0.2363],\n",
       "        [ 0.0876, -0.0845, -0.2818, -0.1904,  0.1882],\n",
       "        [-0.2267, -0.3101,  0.0753, -0.2404, -0.3339]], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----------- 1. node features -------------- #\n",
    "node_embed = nn.Embedding(g.number_of_nodes(), 5)  # 각 노드는 5차원의 임베딩을 가지고 있습니다.\n",
    "# Copy node embeddings to GPU\n",
    "node_embed = node_embed.to('cuda:0')\n",
    "inputs = node_embed.weight                         # 노드 피처로써 이 임베딩 가중치를 사용합니다.\n",
    "nn.init.xavier_uniform_(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "커뮤니티의 라벨은 `'club'`이라는 노드 피처에 저장되어 있습니다. (0은 instructor의 커뮤니티, 1은 club president의 커뮤니티).  \n",
    "오로지 0과 33번 노드에만 라벨링 되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels tensor([0, 1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "labels = g.ndata['club']\n",
    "labeled_nodes = [0, 33]\n",
    "print('Labels', labels[labeled_nodes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GraphSAGE 모델 정의하기\n",
    "\n",
    "우리의 모델은 2개 레이어로 구성되어 있는데, 각각 새로운 노드 표현(representation)을 이웃의 정보를 통합함으로써 계산합니다.  \n",
    "수식은 다음과 같습니다.  \n",
    "\n",
    "\n",
    "$$\n",
    "h_{\\mathcal{N}(v)}^k\\leftarrow \\text{AGGREGATE}_k\\{h_u^{k-1},\\forall u\\in\\mathcal{N}(v)\\}\n",
    "$$\n",
    "\n",
    "$$\n",
    "h_v^k\\leftarrow \\sigma\\left(W^k\\cdot \\text{CONCAT}(h_v^{k-1}, h_{\\mathcal{N}(v)}^k) \\right)\n",
    "$$\n",
    "\n",
    "DGL은 많은 유명한 이웃 통합(neighbor aggregation) 모듈의 구현체를 제공합니다. 모두 쉽게 한 줄의 코드로 호출하여 사용할 수 있습니다.  \n",
    "지원되는 모델의 전체 리스트는 [graph convolution modules](https://docs.dgl.ai/api/python/nn.pytorch.html#module-dgl.nn.pytorch.conv)에서 보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.nn import SAGEConv\n",
    "\n",
    "# ----------- 2. create model -------------- #\n",
    "# 2개의 레이어를 가진 GraphSAGE 모델 구축\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats, 'mean')\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes, 'mean')\n",
    "    \n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h\n",
    "    \n",
    "# 주어진 차원의 모델 생성\n",
    "# 인풋 레이어 차원: 5, 노드 임베딩\n",
    "# 히든 레이어 차원: 16\n",
    "# 아웃풋 레이어 차원: 2, 클래스가 2개 있기 때문, 0과 1\n",
    "\n",
    "net = GraphSAGE(5, 16, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "네트워크를 GPU에 복사함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.to('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.8203792572021484\n",
      "In epoch 5, loss: 0.4111963212490082\n",
      "In epoch 10, loss: 0.21308189630508423\n",
      "In epoch 15, loss: 0.08275498449802399\n",
      "In epoch 20, loss: 0.03401576727628708\n",
      "In epoch 25, loss: 0.01440766267478466\n",
      "In epoch 30, loss: 0.006833690218627453\n",
      "In epoch 35, loss: 0.0037461717147380114\n",
      "In epoch 40, loss: 0.0023471189197152853\n",
      "In epoch 45, loss: 0.0016530591528862715\n",
      "In epoch 50, loss: 0.0012706018751487136\n",
      "In epoch 55, loss: 0.0010407611262053251\n",
      "In epoch 60, loss: 0.0008915828075259924\n",
      "In epoch 65, loss: 0.0007876282907091081\n",
      "In epoch 70, loss: 0.000710575666744262\n",
      "In epoch 75, loss: 0.0006503689801320434\n",
      "In epoch 80, loss: 0.000602009822614491\n",
      "In epoch 85, loss: 0.0005602584569714963\n",
      "In epoch 90, loss: 0.0005215413984842598\n",
      "In epoch 95, loss: 0.000484131567645818\n"
     ]
    }
   ],
   "source": [
    "# ----------- 3. set up loss and optimizer -------------- #\n",
    "# 이 경우, 학습 루프의 손실\n",
    "\n",
    "optimizer = torch.optim.Adam(itertools.chain(net.parameters(), node_embed.parameters()), lr=0.01)\n",
    "\n",
    "# ----------- 4. training -------------------------------- #\n",
    "all_logits = []\n",
    "for e in range(100):\n",
    "    # forward\n",
    "    logits = net(g, inputs)\n",
    "    \n",
    "    # 손실 계산\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels[labeled_nodes])\n",
    "    \n",
    "    # backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    all_logits.append(logits.detach())\n",
    "    \n",
    "    if e % 5 == 0:\n",
    "        print('In epoch {}, loss: {}'.format(e, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.9411764705882353\n"
     ]
    }
   ],
   "source": [
    "# ----------- 5. check results ------------------------ #\n",
    "pred = torch.argmax(logits, axis=1)\n",
    "print('Accuracy', (pred == labels).sum().item() / len(pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**한 GPU 메모리에 그래프와 피처 데이터를 적재할 수 없으면 어떻게 하나요?** \n",
    "\n",
    "* GNN을 천제 그래프에 대해 수행하는 대신에, 몇몇 subgraph에 대해 수행해 수렴시켜보세요.\n",
    "* 다른 샘플을 다른 GPU에 올림으로써 더 빠른 가속을 경험해 보세요.\n",
    "* 그래프를 여러 머신에 분할하여 분산된 형태로 학습시켜보세요.\n",
    "\n",
    "추후에 이러한 방법론을 각각 살펴볼 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
